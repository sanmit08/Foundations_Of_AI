{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train labels: (2648,) val: (662,) test: (1435,)\n",
      "0.3007692307692308 0.315\n",
      "0.3007692307692308 0.315\n",
      "0.3007692307692308 0.315\n",
      "0.3007692307692308 0.315\n",
      "0.3007692307692308 0.315\n",
      "0.3007692307692308 0.315\n",
      "0.3007692307692308 0.315\n",
      "0.34423076923076923 0.33166666666666667\n",
      "0.35115384615384615 0.3383333333333333\n",
      "0.3465384615384615 0.35\n",
      "0.34115384615384614 0.34\n",
      "0.35346153846153844 0.3433333333333333\n",
      "0.35346153846153844 0.345\n",
      "0.35807692307692307 0.33166666666666667\n",
      "0.36153846153846153 0.3333333333333333\n",
      "0.37538461538461537 0.33666666666666667\n",
      "0.36423076923076925 0.33666666666666667\n",
      "0.3861538461538462 0.3466666666666667\n",
      "0.3696153846153846 0.32\n",
      "0.3934615384615385 0.3333333333333333\n",
      "0.3773076923076923 0.3283333333333333\n",
      "0.3696153846153846 0.31666666666666665\n",
      "0.37615384615384617 0.3283333333333333\n",
      "0.38269230769230766 0.3333333333333333\n",
      "0.38884615384615384 0.33\n",
      "0.4034615384615385 0.3416666666666667\n",
      "0.37884615384615383 0.3383333333333333\n",
      "0.3869230769230769 0.345\n",
      "0.4103846153846154 0.35833333333333334\n",
      "0.41346153846153844 0.3466666666666667\n",
      "0.4023076923076923 0.32166666666666666\n",
      "0.39153846153846156 0.36666666666666664\n",
      "0.39884615384615385 0.375\n",
      "0.3942307692307692 0.38\n",
      "0.42653846153846153 0.385\n",
      "0.4203846153846154 0.385\n",
      "0.4288461538461538 0.365\n",
      "0.4196153846153846 0.375\n",
      "0.42346153846153844 0.365\n",
      "0.4369230769230769 0.4116666666666667\n",
      "0.4296153846153846 0.38333333333333336\n",
      "0.43846153846153846 0.41333333333333333\n",
      "0.435 0.4033333333333333\n",
      "0.44 0.4066666666666667\n",
      "0.4257692307692308 0.39666666666666667\n",
      "0.44076923076923075 0.38333333333333336\n",
      "0.44769230769230767 0.41333333333333333\n",
      "0.45384615384615384 0.3983333333333333\n",
      "0.4461538461538462 0.42333333333333334\n",
      "0.44769230769230767 0.415\n",
      "0.44961538461538464 0.43\n",
      "0.4542307692307692 0.44\n",
      "0.42846153846153845 0.415\n",
      "0.46307692307692305 0.43833333333333335\n",
      "0.45384615384615384 0.43\n",
      "0.4346153846153846 0.38166666666666665\n",
      "0.465 0.435\n",
      "0.4419230769230769 0.39666666666666667\n",
      "0.4376923076923077 0.4116666666666667\n",
      "0.4553846153846154 0.43\n",
      "0.4715384615384615 0.4533333333333333\n",
      "0.4257692307692308 0.3933333333333333\n",
      "0.43846153846153846 0.3983333333333333\n",
      "0.4238461538461539 0.39666666666666667\n",
      "0.44961538461538464 0.42333333333333334\n",
      "0.46576923076923077 0.44\n",
      "0.4634615384615385 0.43\n",
      "0.4826923076923077 0.44666666666666666\n",
      "0.475 0.44666666666666666\n",
      "0.46115384615384614 0.41833333333333333\n",
      "0.465 0.43833333333333335\n",
      "0.47 0.43\n",
      "0.44884615384615384 0.42\n",
      "0.4626923076923077 0.425\n",
      "0.4626923076923077 0.425\n",
      "0.4776923076923077 0.44333333333333336\n",
      "0.39 0.36666666666666664\n",
      "0.4492307692307692 0.4266666666666667\n",
      "0.4323076923076923 0.37833333333333335\n",
      "0.45692307692307693 0.415\n",
      "0.4584615384615385 0.4166666666666667\n",
      "0.47807692307692307 0.445\n",
      "0.47115384615384615 0.4533333333333333\n",
      "0.4715384615384615 0.44\n",
      "0.44961538461538464 0.4166666666666667\n",
      "0.4634615384615385 0.43\n",
      "0.44384615384615383 0.4083333333333333\n",
      "0.4684615384615385 0.42833333333333334\n",
      "0.4765384615384615 0.43833333333333335\n",
      "0.4276923076923077 0.39166666666666666\n",
      "0.47 0.43833333333333335\n",
      "0.43538461538461537 0.375\n",
      "0.45269230769230767 0.41833333333333333\n",
      "0.47192307692307695 0.43333333333333335\n",
      "0.45692307692307693 0.4266666666666667\n",
      "0.4846153846153846 0.45166666666666666\n",
      "0.455 0.41833333333333333\n",
      "0.4546153846153846 0.39666666666666667\n",
      "0.48 0.4483333333333333\n",
      "0.45269230769230767 0.38666666666666666\n",
      "0.47307692307692306 0.45666666666666667\n",
      "0.4857692307692308 0.43666666666666665\n",
      "0.48653846153846153 0.46166666666666667\n",
      "0.46576923076923077 0.42333333333333334\n",
      "0.47115384615384615 0.4266666666666667\n",
      "0.48307692307692307 0.43833333333333335\n",
      "0.475 0.43333333333333335\n",
      "0.4469230769230769 0.415\n",
      "0.49538461538461537 0.4583333333333333\n",
      "0.41923076923076924 0.38166666666666665\n",
      "0.47307692307692306 0.435\n",
      "0.4807692307692308 0.43666666666666665\n",
      "0.4461538461538462 0.38666666666666666\n",
      "0.45692307692307693 0.43\n",
      "0.48846153846153845 0.44166666666666665\n",
      "0.4423076923076923 0.3883333333333333\n",
      "0.4869230769230769 0.46\n",
      "0.48346153846153844 0.45\n",
      "0.4734615384615385 0.42\n",
      "0.4888461538461538 0.445\n",
      "0.4930769230769231 0.4483333333333333\n",
      "0.49115384615384616 0.45166666666666666\n",
      "0.4753846153846154 0.44166666666666665\n",
      "0.47692307692307695 0.42833333333333334\n",
      "0.4869230769230769 0.45166666666666666\n",
      "0.4707692307692308 0.4266666666666667\n",
      "0.49153846153846154 0.45\n",
      "0.4753846153846154 0.43333333333333335\n",
      "0.45884615384615385 0.42833333333333334\n",
      "0.4803846153846154 0.44666666666666666\n",
      "0.4519230769230769 0.42833333333333334\n",
      "0.49346153846153845 0.4483333333333333\n",
      "0.4696153846153846 0.42833333333333334\n",
      "0.48730769230769233 0.45\n",
      "0.4803846153846154 0.41\n",
      "0.4646153846153846 0.4166666666666667\n",
      "0.46192307692307694 0.41333333333333333\n",
      "0.43961538461538463 0.385\n",
      "0.4369230769230769 0.36666666666666664\n",
      "0.47192307692307695 0.39666666666666667\n",
      "0.5038461538461538 0.445\n",
      "0.48384615384615387 0.42333333333333334\n",
      "0.4503846153846154 0.395\n",
      "0.4969230769230769 0.44333333333333336\n",
      "0.4957692307692308 0.4583333333333333\n",
      "0.5 0.4533333333333333\n",
      "0.475 0.4166666666666667\n",
      "0.47884615384615387 0.425\n",
      "0.4823076923076923 0.4116666666666667\n",
      "0.47923076923076924 0.4116666666666667\n",
      "0.5069230769230769 0.45\n",
      "0.49346153846153845 0.445\n",
      "0.49423076923076925 0.445\n",
      "0.49346153846153845 0.435\n",
      "0.49038461538461536 0.45\n",
      "0.5065384615384615 0.44\n",
      "0.5026923076923077 0.4483333333333333\n",
      "0.4938461538461538 0.4533333333333333\n",
      "0.4773076923076923 0.41\n",
      "0.4815384615384615 0.425\n",
      "0.5003846153846154 0.45\n",
      "0.4430769230769231 0.395\n",
      "0.48538461538461536 0.39666666666666667\n",
      "0.4973076923076923 0.44\n",
      "0.5038461538461538 0.44333333333333336\n",
      "0.4673076923076923 0.39166666666666666\n",
      "0.475 0.4\n",
      "0.4926923076923077 0.43\n",
      "0.49615384615384617 0.445\n",
      "0.5026923076923077 0.4483333333333333\n",
      "0.5046153846153846 0.44666666666666666\n",
      "0.46615384615384614 0.405\n",
      "0.5034615384615385 0.44\n",
      "0.45269230769230767 0.40166666666666667\n",
      "0.5003846153846154 0.445\n",
      "0.43961538461538463 0.385\n",
      "0.4919230769230769 0.43166666666666664\n",
      "0.48 0.4116666666666667\n",
      "0.4957692307692308 0.43166666666666664\n",
      "0.5076923076923077 0.45\n",
      "0.5076923076923077 0.45666666666666667\n",
      "0.4869230769230769 0.40166666666666667\n",
      "0.4757692307692308 0.40166666666666667\n",
      "0.48384615384615387 0.42\n",
      "0.49923076923076926 0.455\n",
      "0.5065384615384615 0.45166666666666666\n",
      "0.49423076923076925 0.42333333333333334\n",
      "0.4723076923076923 0.4166666666666667\n",
      "0.5015384615384615 0.4583333333333333\n",
      "0.5096153846153846 0.43833333333333335\n",
      "0.5057692307692307 0.43666666666666665\n",
      "0.5096153846153846 0.46166666666666667\n",
      "0.5107692307692308 0.43833333333333335\n",
      "0.4976923076923077 0.43833333333333335\n",
      "0.5080769230769231 0.4533333333333333\n",
      "0.5042307692307693 0.43666666666666665\n",
      "0.5134615384615384 0.4583333333333333\n",
      "0.47884615384615387 0.405\n",
      "0.5161538461538462 0.455\n",
      "0.4642307692307692 0.4\n",
      "0.46153846153846156 0.38666666666666666\n",
      "0.48615384615384616 0.43166666666666664\n",
      "0.5088461538461538 0.45166666666666666\n",
      "0.5061538461538462 0.44333333333333336\n",
      "0.49538461538461537 0.43166666666666664\n",
      "0.5042307692307693 0.4533333333333333\n",
      "0.4815384615384615 0.4066666666666667\n",
      "0.5003846153846154 0.42833333333333334\n",
      "0.49961538461538463 0.43333333333333335\n",
      "0.5080769230769231 0.45166666666666666\n",
      "0.5038461538461538 0.44666666666666666\n",
      "0.5123076923076924 0.4483333333333333\n",
      "0.49923076923076926 0.425\n",
      "0.5161538461538462 0.45\n",
      "0.5065384615384615 0.445\n",
      "0.5146153846153846 0.43833333333333335\n",
      "0.505 0.455\n",
      "0.48846153846153845 0.4266666666666667\n",
      "0.5057692307692307 0.42833333333333334\n",
      "0.5107692307692308 0.43166666666666664\n",
      "0.4938461538461538 0.42333333333333334\n",
      "0.5138461538461538 0.445\n",
      "0.5138461538461538 0.455\n",
      "0.49 0.41833333333333333\n",
      "0.5038461538461538 0.44166666666666665\n",
      "0.5003846153846154 0.42833333333333334\n",
      "0.5030769230769231 0.45\n",
      "0.5157692307692308 0.4483333333333333\n",
      "0.5069230769230769 0.415\n",
      "0.5161538461538462 0.4483333333333333\n",
      "0.5057692307692307 0.435\n",
      "0.4930769230769231 0.4166666666666667\n",
      "0.5080769230769231 0.45666666666666667\n",
      "0.4930769230769231 0.42833333333333334\n",
      "0.5092307692307693 0.42333333333333334\n",
      "0.5142307692307693 0.4533333333333333\n",
      "0.5130769230769231 0.45166666666666666\n",
      "0.5103846153846154 0.44666666666666666\n",
      "0.5034615384615385 0.44166666666666665\n",
      "0.5011538461538462 0.45666666666666667\n",
      "0.5103846153846154 0.4583333333333333\n",
      "0.5203846153846153 0.4483333333333333\n",
      "0.5180769230769231 0.44\n",
      "0.5146153846153846 0.43\n",
      "0.5188461538461538 0.44\n",
      "0.5138461538461538 0.44333333333333336\n",
      "0.5103846153846154 0.45166666666666666\n",
      "0.5138461538461538 0.455\n",
      "0.5138461538461538 0.445\n",
      "0.5084615384615384 0.4666666666666667\n",
      "0.5069230769230769 0.425\n",
      "0.5176923076923077 0.44166666666666665\n",
      "0.51 0.42833333333333334\n",
      "0.49615384615384617 0.43833333333333335\n",
      "0.5192307692307693 0.45166666666666666\n",
      "0.5030769230769231 0.4483333333333333\n",
      "0.5196153846153846 0.45\n",
      "0.5107692307692308 0.435\n",
      "0.4926923076923077 0.42\n",
      "0.5015384615384615 0.4216666666666667\n",
      "0.5092307692307693 0.43666666666666665\n",
      "0.4665384615384615 0.4\n",
      "0.5084615384615384 0.43666666666666665\n",
      "0.5034615384615385 0.44666666666666666\n",
      "0.5096153846153846 0.445\n",
      "0.5057692307692307 0.43833333333333335\n",
      "0.49961538461538463 0.41833333333333333\n",
      "0.5223076923076924 0.44666666666666666\n",
      "0.5130769230769231 0.43333333333333335\n",
      "0.5226923076923077 0.45666666666666667\n",
      "0.5161538461538462 0.43666666666666665\n",
      "0.48115384615384615 0.39666666666666667\n",
      "0.5138461538461538 0.44333333333333336\n",
      "0.5126923076923077 0.45166666666666666\n",
      "0.5126923076923077 0.45666666666666667\n",
      "0.5211538461538462 0.425\n",
      "0.5057692307692307 0.4483333333333333\n",
      "0.5134615384615384 0.435\n",
      "0.5023076923076923 0.4216666666666667\n",
      "0.49846153846153846 0.4116666666666667\n",
      "0.5057692307692307 0.4216666666666667\n",
      "0.5065384615384615 0.45\n",
      "0.5153846153846153 0.445\n",
      "0.5092307692307693 0.435\n",
      "0.5111538461538462 0.43333333333333335\n",
      "0.5161538461538462 0.43666666666666665\n",
      "0.4969230769230769 0.44166666666666665\n",
      "0.5188461538461538 0.44333333333333336\n",
      "0.5061538461538462 0.425\n",
      "0.5130769230769231 0.44333333333333336\n",
      "0.5092307692307693 0.4533333333333333\n",
      "0.5165384615384615 0.445\n",
      "0.5184615384615384 0.43166666666666664\n",
      "0.5238461538461539 0.44666666666666666\n",
      "0.5107692307692308 0.4266666666666667\n",
      "0.4946153846153846 0.41833333333333333\n",
      "0.49846153846153846 0.4266666666666667\n",
      "0.5230769230769231 0.44333333333333336\n",
      "0.5169230769230769 0.43833333333333335\n",
      "0.5176923076923077 0.44333333333333336\n"
     ]
    }
   ],
   "source": [
    "from copy import deepcopy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def train_test_split_self(data, labels, split_ratio=0.2):\n",
    "    split_ratio = 1 - split_ratio\n",
    "    data.reset_index(drop=True, inplace=True)\n",
    "    labels.reset_index(drop=True, inplace=True)\n",
    "    indices = np.random.permutation(data.shape[0])\n",
    "    split_index = int(split_ratio * data.shape[0])\n",
    "    X_train_split = data.iloc[indices[:split_index]]\n",
    "    y_train_split = labels.iloc[indices[:split_index]]\n",
    "    X_val_split = data.iloc[indices[split_index:]]\n",
    "    y_val_split = labels.iloc[indices[split_index:]]\n",
    "\n",
    "    return X_train_split, X_val_split, y_train_split, y_val_split\n",
    "\n",
    "def read_csv_files_pandas(train_data_path, train_labels_path, test_data_path):\n",
    "\n",
    "    data = pd.read_csv(train_data_path)\n",
    "    labels = pd.read_csv(train_labels_path)\n",
    "    data['BEDS'] = labels['BEDS']\n",
    "\n",
    "    data = data[data['PRICE'] < 16000000.0]\n",
    "    data = data[data['PROPERTYSQFT'] < 35000]\n",
    "    data = data[data['BATH'] < 30]\n",
    "\n",
    "    temp_df = pd.get_dummies(data['SUBLOCALITY'], drop_first=True, dtype=int)\n",
    "    data = pd.concat([data,temp_df],axis=1)\n",
    "    temp_df = pd.get_dummies(data['TYPE'], drop_first=True, dtype=int)\n",
    "    data = pd.concat([data,temp_df],axis=1)\n",
    "\n",
    "    drop_columns = ['BROKERTITLE', 'TYPE', 'ADDRESS',\n",
    "       'STATE', 'MAIN_ADDRESS', 'ADMINISTRATIVE_AREA_LEVEL_2', 'LOCALITY',\n",
    "       'SUBLOCALITY', 'STREET_NAME', 'LONG_NAME', 'FORMATTED_ADDRESS',\n",
    "       'LATITUDE', 'LONGITUDE', 'BEDS']\n",
    "\n",
    "    X = data.drop(columns=drop_columns)\n",
    "    X_org = X.copy()\n",
    "    y = data['BEDS']\n",
    "    X = (X - X.min())/(X.max() - X.min())\n",
    "\n",
    "    train_data, val_data, train_labels, val_labels = train_test_split_self(X, y, 0.2)\n",
    "\n",
    "    train_data.reset_index(drop=True, inplace=True)\n",
    "    train_labels.reset_index(drop=True, inplace=True)\n",
    "    val_data.reset_index(drop=True, inplace=True)\n",
    "    val_labels.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    train_data, val_data, train_labels, val_labels = np.array(train_data), np.array(val_data), np.array(train_labels), np.array(val_labels)\n",
    "\n",
    "    test_data = pd.read_csv(test_data_path)\n",
    "\n",
    "    temp_df = pd.get_dummies(test_data['SUBLOCALITY'], drop_first=True, dtype=int)\n",
    "    test_data = pd.concat([test_data,temp_df],axis=1)\n",
    "    temp_df = pd.get_dummies(test_data['TYPE'], drop_first=True, dtype=int)\n",
    "    test_data = pd.concat([test_data,temp_df],axis=1)\n",
    "\n",
    "    test_data = test_data.reindex(columns = data.columns, fill_value=0)\n",
    "\n",
    "    test_data = test_data.drop(columns=drop_columns)\n",
    "\n",
    "    test_data = (test_data - X_org.min())/(X_org.max() - X_org.min())\n",
    "    test_data = np.array(test_data)\n",
    "    test_labels = np.zeros((test_data.shape[0],))\n",
    "\n",
    "    print('Train labels:', train_labels.shape, 'val:', val_labels.shape, 'test:', test_labels.shape)\n",
    "    return train_data, train_labels, val_data, val_labels, test_data, test_labels\n",
    "\n",
    "class softmax_cross_entropy:\n",
    "    def __init__(self):\n",
    "        self.expand_Y = None\n",
    "        self.calib_logit = None\n",
    "        self.sum_exp_calib_logit = None\n",
    "        self.prob = None\n",
    "\n",
    "    def forward(self, X, Y):\n",
    "        self.expand_Y = np.zeros(X.shape).reshape(-1)\n",
    "        self.expand_Y[Y.astype(int).reshape(-1) + np.arange(X.shape[0]) * X.shape[1]] = 1.0\n",
    "        self.expand_Y = self.expand_Y.reshape(X.shape)\n",
    "        self.calib_logit = X - np.amax(X, axis = 1, keepdims = True)\n",
    "        self.sum_exp_calib_logit = np.sum(np.exp(self.calib_logit), axis = 1, keepdims = True)\n",
    "        self.prob = np.exp(self.calib_logit) / self.sum_exp_calib_logit\n",
    "        forward_output = - np.sum(np.multiply(self.expand_Y, self.calib_logit - np.log(self.sum_exp_calib_logit))) / X.shape[0]\n",
    "        return forward_output\n",
    "\n",
    "    def backward(self, X, Y):\n",
    "        backward_output = - (self.expand_Y - self.prob) / X.shape[0]\n",
    "        return backward_output\n",
    "\n",
    "def predict_label(f):\n",
    "    if f.shape[1] == 1:\n",
    "        return (f > 0).astype(float)\n",
    "    else:\n",
    "        return np.argmax(f, axis=1).astype(float).reshape((f.shape[0], -1))\n",
    "\n",
    "class DataSplit:\n",
    "    def __init__(self, X, Y):\n",
    "        self.X = X\n",
    "        self.Y = Y\n",
    "        self.N, self.d = self.X.shape\n",
    "\n",
    "    def get_entire_dataset(self):\n",
    "        return self.X, self.Y\n",
    "\n",
    "    def get_example(self, idx):\n",
    "        batchX = np.zeros((len(idx), self.d))\n",
    "        batchY = np.zeros((len(idx), 1))\n",
    "        for i in range(len(idx)):\n",
    "            batchX[i] = self.X[idx[i]]\n",
    "            batchY[i, :] = self.Y[idx[i]]\n",
    "\n",
    "        return batchX, batchY\n",
    "\n",
    "class linear_layer:\n",
    "    \n",
    "    def __init__(self, input_D, output_D):\n",
    "        self.params = dict()\n",
    "        self.gradient = dict()       \n",
    "        self.params['W'] = np.random.normal(0, 0.1, (input_D, output_D))\n",
    "        self.params['b'] = np.random.normal(0, 0.1, (1, output_D))\n",
    "\n",
    "        self.gradient['W'] = np.zeros((input_D, output_D))\n",
    "        self.gradient['b'] = np.zeros((1, output_D))\n",
    "\n",
    "    def forward(self, X):\n",
    "        forward_output = X@self.params['W'] + self.params['b']\n",
    "        return forward_output\n",
    "\n",
    "    def backward(self, X, grad):\n",
    "        self.gradient['W'] = np.dot(X.T, grad)\n",
    "        self.gradient['b'] = np.sum(grad, axis=0, keepdims=False)\n",
    "        backward_output = np.dot(grad, self.params['W'].T)\n",
    "        return backward_output\n",
    "\n",
    "class relu:\n",
    "    def __init__(self):\n",
    "        self.mask = None\n",
    "\n",
    "    def forward(self, X):\n",
    "        zerosss = np.zeros(X.shape)\n",
    "        forward_output = np.maximum(zerosss, X)\n",
    "        return forward_output\n",
    "\n",
    "    def backward(self, X, grad):\n",
    "        backward_output = grad * (X > 0)\n",
    "        return backward_output\n",
    "\n",
    "def miniBatchGradientDescent(model, _learning_rate):\n",
    "    for module_name, module in model.items():\n",
    "        if hasattr(module, 'params'):\n",
    "            for key, _ in module.params.items():\n",
    "                g = module.gradient[key]\n",
    "                module.params[key] -= _learning_rate * g\n",
    "\n",
    "    return model\n",
    "\n",
    "def forward_pass(model, x, y):\n",
    "    a1 = model['L1'].forward(x)\n",
    "    h1 = model['nonlinear1'].forward(a1)\n",
    "    a2 = model['L2'].forward(h1)\n",
    "    h2 = model['nonlinear2'].forward(a2)\n",
    "    a3 = model['L3'].forward(h2)\n",
    "    loss = model['loss'].forward(a3, y)\n",
    "\n",
    "    return a1, h1, a2, h2, a3, loss\n",
    "\n",
    "def backward_pass(model, x, a1, h1, a2, h2, a3, y):\n",
    "    grad_a3 = model['loss'].backward(a3, y)\n",
    "    grad_h2 = model['L3'].backward(h2, grad_a3)\n",
    "    grad_a2 = model['nonlinear2'].backward(a2, grad_h2)\n",
    "    grad_h1 = model['L2'].backward(h1, grad_a2)\n",
    "    grad_a1 = model['nonlinear1'].backward(a1, grad_h1)\n",
    "    grad_x = model['L1'].backward(x, grad_a1)\n",
    "\n",
    "def compute_accuracy_loss(N_data, DataSet, model, minibatch_size=100):\n",
    "    acc = 0.0\n",
    "    loss = 0.0\n",
    "    count = 0\n",
    "\n",
    "    actual_values = []\n",
    "    predicted_values = []\n",
    "\n",
    "    for i in range(int(np.floor(N_data / minibatch_size))):\n",
    "        x, y = DataSet.get_example(np.arange(i * minibatch_size, (i + 1) * minibatch_size))\n",
    "        _, _, _, _, a2, batch_loss = forward_pass(model, x, y)\n",
    "        loss += batch_loss\n",
    "        acc += np.sum(predict_label(a2) == y)\n",
    "        predicted_values.append(predict_label(a2))\n",
    "        actual_values.append(y)\n",
    "        count += len(y)\n",
    "\n",
    "    return acc / count, loss, actual_values, predicted_values\n",
    "\n",
    "def compute_accuracy_loss_test(N_data, DataSet, model, minibatch_size=1):\n",
    "    acc = 0.0\n",
    "    loss = 0.0\n",
    "    count = 0\n",
    "    actual_values = []\n",
    "    predicted_values = []\n",
    "    x, y = DataSet.get_entire_dataset()\n",
    "\n",
    "    _, _, _, _, a2, batch_loss = forward_pass(model, x, y)\n",
    "    predicted_values.append(predict_label(a2))\n",
    "    return 0, loss, actual_values, predicted_values\n",
    "\n",
    "\n",
    "train_data_path = '/Users/sanmitpatil/Library/CloudStorage/GoogleDrive-sanmitpa@usc.edu/My Drive/USC_CS_AI/CS561_AI/HW/HW3_NeuralNetworks/data/train_data4.csv'\n",
    "train_labels_path = '/Users/sanmitpatil/Library/CloudStorage/GoogleDrive-sanmitpa@usc.edu/My Drive/USC_CS_AI/CS561_AI/HW/HW3_NeuralNetworks/data/train_label4.csv'\n",
    "test_data_path = '/Users/sanmitpatil/Library/CloudStorage/GoogleDrive-sanmitpa@usc.edu/My Drive/USC_CS_AI/CS561_AI/HW/HW3_NeuralNetworks/data/test_data4.csv'\n",
    "\n",
    "# train_data_path = 'train_data.csv'\n",
    "# train_labels_path = 'train_label.csv'\n",
    "# test_data_path = 'test_data.csv'\n",
    "\n",
    "Xtrain, Ytrain, Xval, Yval, Xtest, Ytest = read_csv_files_pandas(train_data_path, train_labels_path, test_data_path)\n",
    "N_train, d = Xtrain.shape\n",
    "N_val, N_test = Xval.shape[0], Xtest.shape[0]\n",
    "\n",
    "trainSet, valSet, testSet = DataSplit(Xtrain, Ytrain), DataSplit(Xval, Yval), DataSplit(Xtest, Ytest)\n",
    "\n",
    "model = dict()\n",
    "num_L1, num_L2, out_L = 48, 48, 50\n",
    "\n",
    "num_epoch, minibatch_size, _learning_rate = 300, 16, 0.015\n",
    "\n",
    "train_acc_record, train_loss_record, val_acc_record, val_loss_record = [], [], [], []\n",
    "best_epoch, best_model = 0, None\n",
    "\n",
    "model['L1'] = linear_layer(input_D=d, output_D=num_L1)\n",
    "model['nonlinear1'] = relu()\n",
    "model['L2'] = linear_layer(input_D=num_L1, output_D=num_L2)\n",
    "model['nonlinear2'] = relu()\n",
    "model['L3'] = linear_layer(input_D=num_L2, output_D=out_L)\n",
    "model['loss'] = softmax_cross_entropy()\n",
    "\n",
    "final_test_accuracies, last_test_accuracies = [], []\n",
    "\n",
    "for t in range(num_epoch):\n",
    "    idx_order = np.random.permutation(N_train)\n",
    "    for i in range(int(np.floor(N_train / minibatch_size))):\n",
    "        x, y = trainSet.get_example(idx_order[i * minibatch_size: (i + 1) * minibatch_size])\n",
    "        a1, h1, a2, h2, a3, _ = forward_pass(model, x, y)\n",
    "        backward_pass(model, x, a1, h1, a2, h2, a3, y)\n",
    "        model = miniBatchGradientDescent(model, _learning_rate)\n",
    "\n",
    "    train_acc, train_loss, _, __ = compute_accuracy_loss(N_train, trainSet, model)\n",
    "    train_acc_record.append(train_acc)\n",
    "    train_loss_record.append(train_loss)\n",
    "\n",
    "    val_acc, val_loss, _, __ = compute_accuracy_loss(N_val, valSet, model)\n",
    "    val_acc_record.append(val_acc)\n",
    "    val_loss_record.append(val_loss)\n",
    "\n",
    "    print(train_acc, val_acc)\n",
    "\n",
    "    latest_model = deepcopy(model)\n",
    "    if val_acc == max(val_acc_record):\n",
    "        best_model = deepcopy(model)\n",
    "        best_epoch = t + 1\n",
    "\n",
    "test_acc, test_loss, actual_values, predicted_values = compute_accuracy_loss_test(N_test, testSet, best_model)\n",
    "\n",
    "final_output = []\n",
    "for batchh in predicted_values:\n",
    "    for val in batchh:\n",
    "        final_output.append(int(val[0]))\n",
    "\n",
    "output_path = 'output.csv'\n",
    "out_df = pd.DataFrame()\n",
    "out_df['BEDS'] = final_output\n",
    "out_df.to_csv(output_path, index=False)\n",
    "\n",
    "# USE_DATASET_SPLIT 4"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
